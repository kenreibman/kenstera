---
phase: 05-verification
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/05-verification/05-VERIFICATION.md
autonomous: true
requirements:
  - SEC-01
must_haves:
  truths:
    - "The string RETELL_API_KEY does not appear anywhere under .next/static/ after a production build"
    - "The actual API key value does not appear anywhere under .next/static/ after a production build"
    - "A VERIFICATION.md checklist exists with all test rows ready for manual execution"
  artifacts:
    - path: ".next/static/"
      provides: "Production client-side bundles free of API key leaks"
    - path: ".planning/phases/05-verification/05-VERIFICATION.md"
      provides: "Verification checklist with key-leak test results and remaining test templates"
  key_links:
    - from: "lib/retell/client.ts"
      to: "app/api/demo-call/route.ts"
      via: "server-only import chain"
      pattern: "import.*retell/client"
---

<objective>
Run a production build, verify no API key leaks in client bundles, and create the VERIFICATION.md checklist with the key-leak results filled in and remaining manual tests templated.

Purpose: SEC-01 requires the Retell API key never appear in client bundles. This plan proves it empirically via grep after `next build`. It also creates the VERIFICATION.md scaffold so Plan 02 can fill in browser-based test results.

Output: `.planning/phases/05-verification/05-VERIFICATION.md` with key-leak test pass/fail recorded and all remaining test rows templated.
</objective>

<execution_context>
@C:/Users/DESKTOP/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/DESKTOP/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-verification/05-RESEARCH.md
@app/api/demo-call/route.ts
@lib/retell/client.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Production build and API key leak check</name>
  <files>.next/static/</files>
  <action>
Run `npm run build` from the project root to produce the production `.next/` directory.

After the build completes successfully, run these grep checks in bash:

1. Check for the env var NAME in client bundles:
   ```
   grep -r "RETELL_API_KEY" .next/static/
   ```
   Expected: no output, exit code 1 (no matches).

2. Check for the actual key VALUE prefix in client bundles (use the first 8 characters of the key from .env.local — read the key first, extract the prefix):
   ```
   # Read first 8 chars of RETELL_API_KEY from .env.local
   KEY_PREFIX=$(grep RETELL_API_KEY .env.local | cut -d= -f2 | head -c 8)
   grep -r "$KEY_PREFIX" .next/static/
   ```
   Expected: no output, exit code 1 (no matches).

3. Sanity check — confirm the key IS present in server bundles (proves grep works):
   ```
   grep -r "RETELL_API_KEY" .next/server/
   ```
   Expected: at least one match (server-side code legitimately references the env var name).

Record the results (pass/fail, timestamp) for inclusion in VERIFICATION.md.

If either client-side grep returns matches, that is a CRITICAL FAILURE. Do not proceed — investigate which file contains the leak and report it.
  </action>
  <verify>
`npm run build` exits with code 0 (no build errors).
`grep -r "RETELL_API_KEY" .next/static/` returns exit code 1 (no matches).
`grep -r "$KEY_PREFIX" .next/static/` returns exit code 1 (no matches).
  </verify>
  <done>Production build succeeds and both key-leak grep checks return zero matches against .next/static/</done>
</task>

<task type="auto">
  <name>Task 2: Create VERIFICATION.md checklist with key-leak results</name>
  <files>.planning/phases/05-verification/05-VERIFICATION.md</files>
  <action>
Create `.planning/phases/05-verification/05-VERIFICATION.md` with the following structure:

```markdown
# Phase 5: Verification Results

**Environment:** localhost:3000 (production build via `next build && next start`)
**Date:** {current date}
**Tester:** {human + Claude}

## Prerequisites

- [x] Production build completed: `npm run build` — exit code 0
- [ ] Production server running: `npm start` — localhost:3000 accessible

## Test Results

### Test 1: API Key Leak Check (SEC-01)

| Check | Result | Timestamp | Environment |
|-------|--------|-----------|-------------|
| `grep -r "RETELL_API_KEY" .next/static/` returns no output | {PASS or FAIL from Task 1} | {timestamp from Task 1} | localhost (post-build) |
| `grep -r "{key_prefix}" .next/static/` returns no output | {PASS or FAIL from Task 1} | {timestamp from Task 1} | localhost (post-build) |
| Server bundle sanity: `grep -r "RETELL_API_KEY" .next/server/` returns matches | {PASS or FAIL from Task 1} | {timestamp from Task 1} | localhost (post-build) |

### Test 2: Call Duration Cap (SEC-04)

| Check | Result | Timestamp | Environment |
|-------|--------|-----------|-------------|
| Submit form with real US phone number | [ ] | | localhost:3000 |
| Answer call and wait without speaking | [ ] | | |
| Call terminates at or before 120 seconds from connection (±5s tolerance) | [ ] | | |

**Procedure:**
1. Open http://localhost:3000 in browser
2. Scroll to demo form section
3. Enter a real name, US phone number, and email
4. Click submit
5. Start stopwatch when phone begins ringing (not when form is submitted)
6. Answer the call and wait silently
7. Record the time when the call automatically disconnects
8. Pass if call disconnects at or before 125 seconds (120s cap + 5s tolerance)

### Test 3: Rate Limit (SEC-02, SEC-03, UX-03)

| Check | Result | Timestamp | Environment |
|-------|--------|-----------|-------------|
| Second submit from same browser (same IP + phone) returns 429 | [ ] | | localhost:3000 |
| Form displays rate-limit error message in UI | [ ] | | localhost:3000 |
| No second phone rings | [ ] | | |

**Procedure:**
1. After Test 2 completes (call ended), the IP and phone are now rate-limited (10-min window)
2. Fill in the same phone number and submit again
3. Verify: form shows rate-limit error message ("You have already requested a demo call recently. Please try again in 10 minutes." or similar)
4. Verify: no second phone call is received
5. Pass if 429 error renders in UI and no phone rings

**Note:** If retesting, use a different phone number (IP rate limit may still be active). Wait 10 minutes for full reset.

### Test 4: Non-US Phone Validation (FORM-02)

| Check | Result | Timestamp | Environment |
|-------|--------|-----------|-------------|
| Canadian number (416 area code) returns validation error | [ ] | | localhost:3000 |
| Error message displays in form UI | [ ] | | localhost:3000 |
| No call is placed | [ ] | | |

**Procedure:**
1. Enter a Canadian phone number: (416) 555-0100 or similar 416-xxx-xxxx
2. Submit the form
3. Verify: form shows phone validation error ("Please enter a valid US phone number" or similar)
4. Verify: no phone call is received
5. Pass if validation error renders and no call is placed

**Note:** This test does NOT consume a rate-limit token (400 is returned before rate limiting). Can run on an already-rate-limited IP.

### Test 5: Form UX States (UX-01, UX-02, UX-03, UX-04)

| Check | Result | Timestamp | Environment |
|-------|--------|-----------|-------------|
| Loading state: spinner/text shown during API call (observed during Test 2) | [ ] | | localhost:3000 |
| Success state: "Your call is on its way" with CTA (observed after Test 2) | [ ] | | localhost:3000 |
| Rate-limit error state: distinct message for 429 (observed during Test 3) | [ ] | | localhost:3000 |
| Button disabled during submission (observed during Test 2) | [ ] | | localhost:3000 |

**Procedure:**
These states are observed as side effects of Tests 2, 3, and 4. No separate submissions needed.
- Loading (UX-01): Watch submit button during Test 2 — should show spinner and "Calling..." text
- Success (UX-02): After Test 2 call triggers — form transitions to success card
- Rate-limit (UX-03): During Test 3 — form shows rate-limit error
- Button disabled (UX-04): During Test 2 submission — button is not clickable

## Summary

| Test | Requirement(s) | Result |
|------|-----------------|--------|
| API Key Leak | SEC-01 | {from Task 1} |
| Call Duration Cap | SEC-04 | [ ] |
| Rate Limit | SEC-02, SEC-03, UX-03 | [ ] |
| Non-US Phone Validation | FORM-02 | [ ] |
| Form UX States | UX-01, UX-02, UX-03, UX-04 | [ ] |

## Failure Log

{Record any failures here with: test name, failure description, fix applied, retest result. Max 3 retests per test.}

---
*Phase: 05-verification*
*Verification date: {date}*
```

Fill in the Test 1 (key-leak) results from Task 1. Use the actual pass/fail values and timestamps. Leave Tests 2-5 as templates with empty checkboxes — Plan 02 will fill those in.
  </action>
  <verify>
File `.planning/phases/05-verification/05-VERIFICATION.md` exists.
Test 1 rows have pass/fail values filled in (not empty checkboxes).
Tests 2-5 have empty checkbox templates ready for Plan 02.
  </verify>
  <done>VERIFICATION.md exists with Test 1 (key-leak) results recorded and Tests 2-5 templated for manual execution</done>
</task>

</tasks>

<verification>
1. `npm run build` completes without errors
2. No client-side key leak: `grep -r "RETELL_API_KEY" .next/static/` returns no matches
3. No client-side key value leak: prefix grep returns no matches
4. VERIFICATION.md exists with Test 1 results filled in
</verification>

<success_criteria>
- Production build succeeds
- Both API key leak grep checks pass (no matches in .next/static/)
- VERIFICATION.md created with Test 1 results and Tests 2-5 templates
</success_criteria>

<output>
After completion, create `.planning/phases/05-verification/05-01-SUMMARY.md`
</output>
